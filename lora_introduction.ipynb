{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Lora Introduction\n",
    "\n",
    "references:\n",
    "- https://youtu.be/DhRoTONcyZE?si=V9fBdboA9VJXw9Fx\n",
    "- https://youtu.be/PXWYUTMt-AU?si=Nj8mpWCR20TQmnQ2\n"
   ],
   "id": "5f30eea2efc22e45"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<img src=\"attachments/decomposed.png\" width=\"1000\">",
   "id": "1e5f82d85144d442"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Generate a rank-deficient matrix W",
   "id": "228e55ffd79fa38d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-05T11:42:18.553416Z",
     "start_time": "2025-12-05T11:42:18.547536Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "_ = torch.manual_seed(42)\n",
    "\n",
    "d, k = 10, 10 # d=input dimension, k=output dimension\n",
    "\n",
    "W_rank = 2 # W_rank < d & W_rank < k\n",
    "B, A = torch.rand(d, W_rank), torch.randn(W_rank, k)\n",
    "W = B @ A\n",
    "print(\"W shape: \", W.shape)\n",
    "print(\"B & A shapes: \", B.shape, A.shape)\n",
    "\n",
    "print(\"Total parameters of W: \", W.nelement())\n",
    "print(\"Total parameters of B & A: \", B.nelement() + A.nelement())"
   ],
   "id": "5eb1aefceff3b9be",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W shape:  torch.Size([10, 10])\n",
      "B & A shapes:  torch.Size([10, 2]) torch.Size([2, 10])\n",
      "Total parameters of W:  100\n",
      "Total parameters of B & A:  40\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### B & A are not unique. And is possible to decompose them without the original B & A",
   "id": "6264c6736d692451"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-05T11:20:06.002722Z",
     "start_time": "2025-12-05T11:20:06.000563Z"
    }
   },
   "cell_type": "code",
   "source": [
    "W_rank = np.linalg.matrix_rank(W)\n",
    "print(W_rank)"
   ],
   "id": "12bcf26d84d37595",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Calculate SVD decomposition of the W matrix.",
   "id": "1889978ac5fbd319"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-05T11:45:41.727850Z",
     "start_time": "2025-12-05T11:45:41.723633Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def svd(W):\n",
    "    # Perform SVD on W (W = UxSxV^T)\n",
    "    U, S, V = torch.svd(W)\n",
    "\n",
    "    # For rank-r factorization, keep only the first r singular values (and corresponding columns of U & V)\n",
    "    U_r = U[:, :W_rank]\n",
    "    S_r = torch.diag(S[:W_rank])\n",
    "    V_r = V[:, :W_rank].t()\n",
    "\n",
    "    # Compute C = U_r * S_r & V_r\n",
    "    B_ = U_r @ S_r\n",
    "    A_ = V_r\n",
    "    return B_, A_\n",
    "\n",
    "B_, A_ = svd(W)\n",
    "print(f\"Shape of B': {B_.shape}\")\n",
    "print(f\"Shape of A': {A_.shape}\")\n",
    "\n",
    "# they are not the same, but (B @ A) == (B' @ A')\n",
    "print(\"Original B: \\n\", B)\n",
    "print(\"Decomposed B': \\n\", B_)"
   ],
   "id": "86dddbd82372f4f5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of B': torch.Size([10, 2])\n",
      "Shape of A': torch.Size([2, 10])\n",
      "Original B: \n",
      " tensor([[0.8823, 0.9150],\n",
      "        [0.3829, 0.9593],\n",
      "        [0.3904, 0.6009],\n",
      "        [0.2566, 0.7936],\n",
      "        [0.9408, 0.1332],\n",
      "        [0.9346, 0.5936],\n",
      "        [0.8694, 0.5677],\n",
      "        [0.7411, 0.4294],\n",
      "        [0.8854, 0.5739],\n",
      "        [0.2666, 0.6274]])\n",
      "Decomposed B': \n",
      " tensor([[-4.6980, -0.2061],\n",
      "        [-3.6697, -1.4674],\n",
      "        [-2.6475, -0.5715],\n",
      "        [-2.8966, -1.3529],\n",
      "        [-2.5655,  1.8451],\n",
      "        [-3.8867,  0.7025],\n",
      "        [-3.6607,  0.6154],\n",
      "        [-2.9623,  0.6582],\n",
      "        [-3.7158,  0.6372],\n",
      "        [-2.4377, -0.9225]])\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-05T11:20:06.747584Z",
     "start_time": "2025-12-05T11:20:06.743127Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Generate random bias and input\n",
    "bias = torch.randn(d)\n",
    "x = torch.randn(d)\n",
    "\n",
    "# Compute y = Wx + b\n",
    "y = W @ x + bias\n",
    "\n",
    "# Compute y' = CRx + b\n",
    "y_prime = (B @ A) @ x + bias\n",
    "\n",
    "print(\"Original y using W: \\n\", y)\n",
    "print(\"\")\n",
    "print(\"y, computed using BA:\\n\", y_prime)\n",
    "\n",
    "print(f\"Both tensors have the same content: {torch.allclose(W, B @ A, atol=1e-4)}\")"
   ],
   "id": "a1a8502127fcb906",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original y using W: \n",
      " tensor([6.1220, 4.5818, 3.2029, 3.8161, 4.6506, 6.4817, 4.2482, 3.4647, 5.5003,\n",
      "        2.5157])\n",
      "\n",
      "y, computed using BA:\n",
      " tensor([6.1220, 4.5818, 3.2029, 3.8161, 4.6506, 6.4817, 4.2482, 3.4647, 5.5003,\n",
      "        2.5157])\n",
      "Both tensors have the same content: True\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-05T11:21:44.941650Z",
     "start_time": "2025-12-05T11:21:44.938776Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Total parameters of W: \", W.nelement())\n",
    "print(\"Total parameters of B & A: \", B.nelement() + A.nelement())"
   ],
   "id": "7abf96d634be70fb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters of W:  100\n",
      "Total parameters of B & A:  40\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Use Case",
   "id": "327c35396a36db5c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Training\n",
    "\n",
    "1. Finetuning<br>\n",
    "<img src=\"attachments/lora1.png\" width=300><br>\n",
    "\n",
    "a. freeze the original weights (that are of shape [d, k])\n",
    "b. initialise new loRA weights of shape [dxr] & [r, k], where r < d & r < k\n",
    "c. train the new loRA weights for the new downstream task\n",
    "\n",
    "2. Better checkpointing<br>\n",
    "<img src=\"attachments/lora2.png\" width=300><br>\n",
    "\n",
    "a. Instead of saving a entirely new copy of the whole model, we will only save the lora module for checkpoints.\n",
    "b. We can also train the model one multiple tasks in parallel, using seperate lora modules.\n",
    "\n",
    "### Inference\n",
    "\n",
    "1. Model switching<br>\n",
    "<img src=\"attachments/lora3.png\" width=800><br>\n",
    "\n",
    "2. Model Specialisation<br>\n",
    "<img src=\"attachments/lora4.png\" width=300><br>\n",
    "\n",
    "We can train different lora modules and switch between them using the lora modules."
   ],
   "id": "36012544aac16941"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Lora Implementation",
   "id": "42ce8f7b56d327a8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-05T13:17:14.282764Z",
     "start_time": "2025-12-05T13:17:12.409134Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "_ = torch.manual_seed(0)"
   ],
   "id": "8e834beb5221bfd2",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Load Mnist Dataset",
   "id": "7904d67db7e8f470"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-05T13:18:54.996856Z",
     "start_time": "2025-12-05T13:18:43.437524Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from core.utils.device import get_device\n",
    "\n",
    "device = get_device()\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "\n",
    "# Load the MNIST dataset\n",
    "mnist_trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(mnist_trainset, batch_size=10, shuffle=True)\n",
    "\n",
    "# Load the MNIST test set\n",
    "mnist_testset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(mnist_testset, batch_size=10, shuffle=True)"
   ],
   "id": "5c33dd2f70dc05e7",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9.91M/9.91M [00:02<00:00, 3.46MB/s]\n",
      "100%|██████████| 28.9k/28.9k [00:00<00:00, 117kB/s]\n",
      "100%|██████████| 1.65M/1.65M [00:01<00:00, 1.04MB/s]\n",
      "100%|██████████| 4.54k/4.54k [00:00<00:00, 6.06MB/s]\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Create a classifier model (a unnecessarily big net for demonstration purposes).",
   "id": "f3b578262c02866f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-05T13:20:45.301661Z",
     "start_time": "2025-12-05T13:20:45.101737Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MnistClassifier(nn.Module):\n",
    "    def __init__(self, hidden_size_1=1000, hidden_size_2=2000):\n",
    "        super(MnistClassifier,self).__init__()\n",
    "        self.linear1 = nn.Linear(28*28, hidden_size_1)\n",
    "        self.linear2 = nn.Linear(hidden_size_1, hidden_size_2)\n",
    "        self.linear3 = nn.Linear(hidden_size_2, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, img):\n",
    "        x = img.view(-1, 28*28)\n",
    "        x = self.relu(self.linear1(x))\n",
    "        x = self.relu(self.linear2(x))\n",
    "        x = self.linear3(x)\n",
    "        return x\n",
    "\n",
    "net = MnistClassifier().to(device)"
   ],
   "id": "d3ede4d5d86620fd",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Train the network only for 1 epoch to simulate a complete general pre-training on the data",
   "id": "58c400b78dc02374"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-05T13:22:11.670522Z",
     "start_time": "2025-12-05T13:21:19.744750Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train(train_loader, net, epochs=5, total_iterations_limit=None):\n",
    "    cross_el = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "    total_iterations = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        net.train()\n",
    "\n",
    "        loss_sum = 0\n",
    "        num_iterations = 0\n",
    "\n",
    "        data_iterator = tqdm(train_loader, desc=f'Epoch {epoch+1}')\n",
    "        if total_iterations_limit is not None:\n",
    "            data_iterator.total = total_iterations_limit\n",
    "        for data in data_iterator:\n",
    "            num_iterations += 1\n",
    "            total_iterations += 1\n",
    "            x, y = data\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = net(x.view(-1, 28*28))\n",
    "            loss = cross_el(output, y)\n",
    "            loss_sum += loss.item()\n",
    "            avg_loss = loss_sum / num_iterations\n",
    "            data_iterator.set_postfix(loss=avg_loss)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if total_iterations_limit is not None and total_iterations >= total_iterations_limit:\n",
    "                return\n",
    "\n",
    "train(train_loader, net, epochs=1)"
   ],
   "id": "567e9e4e87365477",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 6000/6000 [00:51<00:00, 115.57it/s, loss=0.234]\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Keep a copy of the original weights (cloning them) so later we can prove that a fine-tuning with LoRA doesn't alter the original weights",
   "id": "21d567a7008401f3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-05T13:23:00.749991Z",
     "start_time": "2025-12-05T13:23:00.743749Z"
    }
   },
   "cell_type": "code",
   "source": [
    "original_weights = {}\n",
    "for name, param in net.named_parameters():\n",
    "    original_weights[name] = param.clone().detach()"
   ],
   "id": "4588474481f2070d",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The the performance of the pretrained network. As we can see, the network performs poorly on the digit 4. Let's fine-tune it on the digit 4",
   "id": "7367df8a70d084cb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-05T13:24:21.131757Z",
     "start_time": "2025-12-05T13:24:15.165777Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def test():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    wrong_counts = [0 for i in range(10)]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(test_loader, desc='Testing'):\n",
    "            x, y = data\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            output = net(x.view(-1, 784))\n",
    "            for idx, i in enumerate(output):\n",
    "                if torch.argmax(i) == y[idx]:\n",
    "                    correct +=1\n",
    "                else:\n",
    "                    wrong_counts[y[idx]] +=1\n",
    "                total +=1\n",
    "    print(f'Accuracy: {round(correct/total, 3)}')\n",
    "    for i in range(len(wrong_counts)):\n",
    "        print(f'wrong counts for the digit {i}: {wrong_counts[i]}')\n",
    "\n",
    "test()"
   ],
   "id": "309122f21595f14c",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 1000/1000 [00:05<00:00, 167.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.956\n",
      "wrong counts for the digit 0: 44\n",
      "wrong counts for the digit 1: 51\n",
      "wrong counts for the digit 2: 15\n",
      "wrong counts for the digit 3: 46\n",
      "wrong counts for the digit 4: 76\n",
      "wrong counts for the digit 5: 23\n",
      "wrong counts for the digit 6: 28\n",
      "wrong counts for the digit 7: 56\n",
      "wrong counts for the digit 8: 52\n",
      "wrong counts for the digit 9: 45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's visualize how many parameters are in the original network, before introducing the LoRA matrices.",
   "id": "2fd6feb324b16603"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-05T13:25:22.240543Z",
     "start_time": "2025-12-05T13:25:22.234154Z"
    }
   },
   "cell_type": "code",
   "source": [
    "total_parameters_original = 0\n",
    "for index, layer in enumerate([net.linear1, net.linear2, net.linear3]):\n",
    "    total_parameters_original += layer.weight.nelement() + layer.bias.nelement()\n",
    "    print(f'Layer {index+1}: W: {layer.weight.shape} + B: {layer.bias.shape}')\n",
    "print(f'Total number of parameters: {total_parameters_original:,}')"
   ],
   "id": "4f22f8c51cb5a473",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 1: W: torch.Size([1000, 784]) + B: torch.Size([1000])\n",
      "Layer 2: W: torch.Size([2000, 1000]) + B: torch.Size([2000])\n",
      "Layer 3: W: torch.Size([10, 2000]) + B: torch.Size([10])\n",
      "Total number of parameters: 2,807,010\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Define the LoRA parameterization as described in the paper. The full detail on how PyTorch parameterizations work is here: https://pytorch.org/tutorials/intermediate/parametrizations.html",
   "id": "439912b458adf137"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-05T13:31:46.466859Z",
     "start_time": "2025-12-05T13:31:46.457880Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class LoRAParametrization(nn.Module):\n",
    "    def __init__(self, features_in, features_out, rank=1, alpha=1, device='cpu'):\n",
    "        super().__init__()\n",
    "        # Section 4.1 of the paper:\n",
    "        #   We use a random Gaussian initialization for A and zero for B, so ∆W = BA is zero at the beginning of training\n",
    "        self.lora_A = nn.Parameter(torch.zeros((rank,features_out)).to(device))\n",
    "        self.lora_B = nn.Parameter(torch.zeros((features_in, rank)).to(device))\n",
    "        nn.init.normal_(self.lora_A, mean=0, std=1)\n",
    "\n",
    "        # Section 4.1 of the paper:\n",
    "        #   We then scale ∆Wx by α/r , where α is a constant in r.\n",
    "        #   When optimizing with Adam, tuning α is roughly the same as tuning the learning rate if we scale the initialization appropriately.\n",
    "        #   As a result, we simply set α to the first r we try and do not tune it.\n",
    "        #   This scaling helps to reduce the need to retune hyperparameters when we vary r.\n",
    "        self.scale = alpha / rank\n",
    "        self.enabled = True\n",
    "\n",
    "    def forward(self, original_weights):\n",
    "        if self.enabled:\n",
    "            # Return W + (B*A)*scale\n",
    "            return original_weights + torch.matmul(self.lora_B, self.lora_A).view(original_weights.shape) * self.scale\n",
    "        else:\n",
    "            return original_weights"
   ],
   "id": "dece74134bab17c5",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Add the parameterization to our network.",
   "id": "213ff6f049adfb88"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-05T13:32:55.375863Z",
     "start_time": "2025-12-05T13:32:54.825452Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch.nn.utils.parametrize as parametrize\n",
    "\n",
    "def linear_layer_parameterization(layer, device, rank=1, lora_alpha=1):\n",
    "    # Only add the parameterization to the weight matrix, ignore the Bias\n",
    "\n",
    "    # From section 4.2 of the paper:\n",
    "    #   We limit our study to only adapting the attention weights for downstream tasks and freeze the MLP modules (so they are not trained in downstream tasks) both for simplicity and parameter-efficiency.\n",
    "    #   [...]\n",
    "    #   We leave the empirical investigation of [...], and biases to a future work.\n",
    "\n",
    "    features_in, features_out = layer.weight.shape\n",
    "    return LoRAParametrization(\n",
    "        features_in, features_out, rank=rank, alpha=lora_alpha, device=device\n",
    "    )\n",
    "\n",
    "parametrize.register_parametrization(\n",
    "    net.linear1, \"weight\", linear_layer_parameterization(net.linear1, device)\n",
    ")\n",
    "parametrize.register_parametrization(\n",
    "    net.linear2, \"weight\", linear_layer_parameterization(net.linear2, device)\n",
    ")\n",
    "parametrize.register_parametrization(\n",
    "    net.linear3, \"weight\", linear_layer_parameterization(net.linear3, device)\n",
    ")\n",
    "\n",
    "\n",
    "def enable_disable_lora(enabled=True):\n",
    "    for layer in [net.linear1, net.linear2, net.linear3]:\n",
    "        layer.parametrizations[\"weight\"][0].enabled = enabled"
   ],
   "id": "aa2c9baad0dcbf61",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Display the number of parameters added by LoRA.",
   "id": "ac53e598dafec759"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-05T13:34:18.453941Z",
     "start_time": "2025-12-05T13:34:18.444512Z"
    }
   },
   "cell_type": "code",
   "source": [
    "total_parameters_lora = 0\n",
    "total_parameters_non_lora = 0\n",
    "for index, layer in enumerate([net.linear1, net.linear2, net.linear3]):\n",
    "    total_parameters_lora += layer.parametrizations[\"weight\"][0].lora_A.nelement() + layer.parametrizations[\"weight\"][0].lora_B.nelement()\n",
    "    total_parameters_non_lora += layer.weight.nelement() + layer.bias.nelement()\n",
    "    print(\n",
    "        f'Layer {index+1}: W: {layer.weight.shape} + B: {layer.bias.shape} + Lora_A: {layer.parametrizations[\"weight\"][0].lora_A.shape} + Lora_B: {layer.parametrizations[\"weight\"][0].lora_B.shape}'\n",
    "    )\n",
    "# The non-LoRA parameters count must match the original network\n",
    "assert total_parameters_non_lora == total_parameters_original\n",
    "print(f'Total number of parameters (original): {total_parameters_non_lora:,}')\n",
    "print(f'Total number of parameters (original + LoRA): {total_parameters_lora + total_parameters_non_lora:,}')\n",
    "print(f'Parameters introduced by LoRA: {total_parameters_lora:,}')\n",
    "parameters_incremment = (total_parameters_lora / total_parameters_non_lora) * 100\n",
    "print(f'Parameters incremment: {parameters_incremment:.3f}%')"
   ],
   "id": "561226078668b7bf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 1: W: torch.Size([1000, 784]) + B: torch.Size([1000]) + Lora_A: torch.Size([1, 784]) + Lora_B: torch.Size([1000, 1])\n",
      "Layer 2: W: torch.Size([2000, 1000]) + B: torch.Size([2000]) + Lora_A: torch.Size([1, 1000]) + Lora_B: torch.Size([2000, 1])\n",
      "Layer 3: W: torch.Size([10, 2000]) + B: torch.Size([10]) + Lora_A: torch.Size([1, 2000]) + Lora_B: torch.Size([10, 1])\n",
      "Total number of parameters (original): 2,807,010\n",
      "Total number of parameters (original + LoRA): 2,813,804\n",
      "Parameters introduced by LoRA: 6,794\n",
      "Parameters incremment: 0.242%\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Freeze all the parameters of the original network and only fine tuning the ones introduced by LoRA. Then fine-tune the model on the digit 4 and only for 100 batches.",
   "id": "bfdb18a571aebdfb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-05T13:35:19.753348Z",
     "start_time": "2025-12-05T13:35:17.741632Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Freeze the non-Lora parameters\n",
    "for name, param in net.named_parameters():\n",
    "    if 'lora' not in name:\n",
    "        print(f'Freezing non-LoRA parameter {name}')\n",
    "        param.requires_grad = False\n",
    "\n",
    "# Load the MNIST dataset again, by keeping only the digit 4\n",
    "mnist_trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "exclude_indices = mnist_trainset.targets == 4\n",
    "mnist_trainset.data = mnist_trainset.data[exclude_indices]\n",
    "mnist_trainset.targets = mnist_trainset.targets[exclude_indices]\n",
    "# Create a dataloader for the training\n",
    "train_loader = torch.utils.data.DataLoader(mnist_trainset, batch_size=10, shuffle=True)\n",
    "\n",
    "# Train the network with LoRA only on the digit 4 and only for 100 batches (hoping that it would improve the performance on the digit 4)\n",
    "train(train_loader, net, epochs=1, total_iterations_limit=100)"
   ],
   "id": "5db18750a8f7a5a1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freezing non-LoRA parameter linear1.bias\n",
      "Freezing non-LoRA parameter linear1.parametrizations.weight.original\n",
      "Freezing non-LoRA parameter linear2.bias\n",
      "Freezing non-LoRA parameter linear2.parametrizations.weight.original\n",
      "Freezing non-LoRA parameter linear3.bias\n",
      "Freezing non-LoRA parameter linear3.parametrizations.weight.original\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  99%|█████████▉| 99/100 [00:01<00:00, 50.31it/s, loss=0.168]\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Verify that the fine-tuning didn't alter the original weights, but only the ones introduced by LoRA.",
   "id": "5801edac07d96c0f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-05T13:35:47.436772Z",
     "start_time": "2025-12-05T13:35:47.058838Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Check that the frozen parameters are still unchanged by the finetuning\n",
    "assert torch.all(net.linear1.parametrizations.weight.original == original_weights['linear1.weight'])\n",
    "assert torch.all(net.linear2.parametrizations.weight.original == original_weights['linear2.weight'])\n",
    "assert torch.all(net.linear3.parametrizations.weight.original == original_weights['linear3.weight'])\n",
    "\n",
    "enable_disable_lora(enabled=True)\n",
    "# The new linear1.weight is obtained by the \"forward\" function of our LoRA parametrization\n",
    "# The original weights have been moved to net.linear1.parametrizations.weight.original\n",
    "# More info here: https://pytorch.org/tutorials/intermediate/parametrizations.html#inspecting-a-parametrized-module\n",
    "assert torch.equal(net.linear1.weight, net.linear1.parametrizations.weight.original + (net.linear1.parametrizations.weight[0].lora_B @ net.linear1.parametrizations.weight[0].lora_A) * net.linear1.parametrizations.weight[0].scale)\n",
    "\n",
    "enable_disable_lora(enabled=False)\n",
    "# If we disable LoRA, the linear1.weight is the original one\n",
    "assert torch.equal(net.linear1.weight, original_weights['linear1.weight'])"
   ],
   "id": "37dd8a4af6ff68d4",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Test the network with LoRA enabled (the digit 4 should be classified better)",
   "id": "3d93286a0f1717b5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-05T13:37:11.660270Z",
     "start_time": "2025-12-05T13:37:05.480834Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Test with LoRA enabled (Note, the training made digit 9 worse)\n",
    "enable_disable_lora(enabled=True)\n",
    "test()"
   ],
   "id": "9654ad6c3d42e1d6",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 1000/1000 [00:06<00:00, 162.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.956\n",
      "wrong counts for the digit 0: 46\n",
      "wrong counts for the digit 1: 38\n",
      "wrong counts for the digit 2: 15\n",
      "wrong counts for the digit 3: 52\n",
      "wrong counts for the digit 4: 14\n",
      "wrong counts for the digit 5: 26\n",
      "wrong counts for the digit 6: 21\n",
      "wrong counts for the digit 7: 56\n",
      "wrong counts for the digit 8: 47\n",
      "wrong counts for the digit 9: 129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Test the network with LoRA disabled (the accuracy and errors counts must be the same as the original network)",
   "id": "1d175d5231ca3f03"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-05T13:38:10.794208Z",
     "start_time": "2025-12-05T13:38:05.468168Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Test with LoRA disabled\n",
    "enable_disable_lora(enabled=False)\n",
    "test()"
   ],
   "id": "745236973c3c225",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 1000/1000 [00:05<00:00, 188.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.956\n",
      "wrong counts for the digit 0: 44\n",
      "wrong counts for the digit 1: 51\n",
      "wrong counts for the digit 2: 15\n",
      "wrong counts for the digit 3: 46\n",
      "wrong counts for the digit 4: 76\n",
      "wrong counts for the digit 5: 23\n",
      "wrong counts for the digit 6: 28\n",
      "wrong counts for the digit 7: 56\n",
      "wrong counts for the digit 8: 52\n",
      "wrong counts for the digit 9: 45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 14
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
